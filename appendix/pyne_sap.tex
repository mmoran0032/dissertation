\chapter{ANALYSIS PACKAGE}
\label{ch:pyne}

The analysis and plots contained within this dissertation were completed
using Python and a small set of standard scientific python packages:


\begin{description}
    \item[Matplotlib] a 2D plotting package that supports multiple
        backends and output formats
    \item[NumPy] a standard numeric package for array (vector and
        matrix) computing
    \item[Pandas] a tabular data interface that organizes heterogeneous
        data into simple to use and manipulate data structures
    \item[PyMC3] a probabilistic programming language in python useful
        for Bayesian inference
    \item[SciPy] a scientific utilities package built on top of NumPy
        that includes general-purpose routines such as curve fitting,
        root finding, signal processing and more
\end{description}

These packages are standard components of the Python Scientific Stack,
and their usage and internals are well-documented and trusted by many
scientists in multiple fields. The usage of Python in the scientific
community has increased steadily over the years, with multiple special
purpose packages built on top of the foundation of these three packages,
particularly NumPy.

For the actual analysis, a two-part analysis framework was developed in
tandem with the analysis work: Python for Nuclear Experiments (PyNE) and
the St. George Analysis Package (SAP). These packages are designed to be
extensible by other research groups, guided by the requirements of the
St. George group, while providing an easy-to-use and understand
object-oriented interface to performing nuclear astrophysics research.
Development work was chronicled on the packages' GitHub
page\footnote{\url{https://github.com/mmoran0032/pyne}}, and may be
installed from there.

These packages would not be possible without two other important
packages: the ROOT\footnote{\url{http://root.cern.ch/}} Data Analysis
Framework\cite{ROOT} and
\texttt{evt2root}\footnote{\url{https://github.com/ksmith0/evt2root}}.


\section{Python for Nuclear Experiments}

The PyNE package provides base functionality for interacting with
experimental data generated through the standard data acquisition
systems in use at the Nuclear Science Laboratory at the University of
Notre Dame. The package was designed to provide a base pythonic
interface to the data files, including converting them into data files
that work easily with standard python scientific packages. The
underlying purpose for PyNE is not to be an end-to-end analysis package,
but provide the necessary structure to such an end-to-end analysis
package with a common and simple API.

The primary functionality is the organizing of experimental data. From
binary files in either (currently) \verb+.Chn+ or \verb+.evt+ format,
the data is converted into \verb+numpy.ndarray+ format. A logical
directory structure is set up, with run metadata extracted from the
binary files stored in a local JSON file.

Within the API, the data can be easily queried and worked with. The
provided \verb+pyne.Data+ objects (subclassed for channel and event
data) implement the iterator protocol over the ADCs. In this way,
concise and clear analysis on the entire 16-strip detector can be
accomplished through code as simple as the following:

\begin{python}
import pyne
from local_analysis import determine_counts

data_obj = pyne.EVTData('data/run0808')
data_obj.load_data()

total_counts = sum(
    determine_counts(d) for d in data_obj.adc
)
\end{python}

By creating objects that encapsulate the data and meta-data, the
end-to-end analysis becomes much easier, as details about a run and
exploratory work are vastly simplified.

\section{St. George Analysis Package}

For most analysis, there is a need for more work than just finding the
total number of counts. For the experiment conducted within this
dissertation, a few routines were standardized within an add-on package
to PyNE called SAP. Since PyNE just provides the primitive
classes for analysis, any number of extension packages may be produced
building upon that core functionality.


\section{Justification}

Python has become a leading programming language across a range of uses
and industry, led primarily by its data analysis, data science, and
machine learning applications\footnote{Why is Python Growing So Quickly?
\url{https://stackoverflow.blog/2017/09/14/python-growing-quickly/}}.
The three main data analysis packages for python\---{}numpy, pandas, and
matplotlib\---{}have become essential tools in a variety of cases,
especially within the sciences. Packages built upon this base, such as
SunPy\footnote{\url{https://github.com/sunpy/sunpy}} (Python for Solar
Physics), AstroPy\footnote{\url{https://github.com/astropy/astropy}},
Biopython\footnote{\url{https://github.com/biopython/biopython}}, and
many others have been written, tested, and used by researchers all
around the globe. Ground-breaking scientific discoveries, such as the
detection of gravitational waves by LIGO, have been made using Python
as one of the primary analysis
languages\footnote{\url{https://losc.ligo.org/s/events/GW150914/GW150914_tutorial.html}}.
Python has become a \emph{de facto} language for data analysis in many
circles (outside of statistics where
R\footnote{https://www.r-project.org/} is still the primary language),
an adopting it for nuclear astrophysics should not be far behind.

For those thinking that the ``slow'' speed of python will hold them back
from performing large-scale analysis, or that python might not fit with
the use cases that the researcher has, I emplore you to watch Jake
VanderPlas's
keynote\footnote{https://www.youtube.com/watch?v=ZyjCqQEUa8o} from PyCon
2017. One additional point is that the speed of execution is most likely
not the limiting factor in the analysis, but the speed of the researcher
to write, debug, and understand the code may very well be that
bottleneck. Additionally, the use of Jupyter
Notebooks\footnote{\url{https://jupyter.org/}} for code alongside
documentation allows for a lower barrier to entry for rerunning and
understanding experimental analyses years later.

As a user of python since 2007, I have seen large changes in the python
ecosystem and have gradually included python more and more into my own
work. Right now, I owe my career to python and the open source data
analysis packages, and I don't expect that fact to change any time soon.
While my work has shifted from curve fitting to gradient boosted trees
and neural networks, it is still in python, and in many cases the final
understanding of my complex models comes down to dot products in
\verb+numpy+ and plotting with \verb+matplotlib+.

This dissertation would not have been possible without the countless
hours of work and throusands of open source contributions to these core
data analysis libraries and python itself.
